{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\FullStack_Data\\\\MACHINE_LEARNING\\\\PROJECTS\\\\Detailed_Project\\\\Detailed_MLFLOW\\\\research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\FullStack_Data\\\\MACHINE_LEARNING\\\\PROJECTS\\\\Detailed_Project\\\\Detailed_MLFLOW'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class Data_Transformation_Config:\n",
    "    root_dir: Path\n",
    "    data_transformation_input: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Detailed_MLFLow_project.constants import *\n",
    "from src.Detailed_MLFLow_project.utils.common import create_directories,read_yaml\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_read=CONFIG_FILE_PATH,schema_read=SCHEMA_FILE_PATH,params_read=PARAMS_FILE_PATH ):\n",
    "        self.config=read_yaml(config_read),\n",
    "        self.schema=read_yaml(schema_read),\n",
    "        self.params=read_yaml(params_read)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def data_transformed_configuration(self)->Data_Transformation_Config:\n",
    "        config=self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "    \n",
    "        get_data_transformed_config=Data_Transformation_Config(\n",
    "            root_dir=config.root_dir,\n",
    "            data_transformation_input=config.unzip_data\n",
    "\n",
    "        )\n",
    "\n",
    "        return get_data_transformed_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Feature_Engineering:\n",
    "    def __init__(self,config:Data_Transformation_Config):\n",
    "        self.config=config\n",
    "\n",
    "    def Random_Sample_imputation(self,data,feature):\n",
    "        random_sample=data[feature].dropna().sample(data[feature].isnull().sum())               \n",
    "        random_sample.index=data[data[feature].isnull()].index\n",
    "        data.loc[data[feature].isnull(),feature]=random_sample\n",
    "\n",
    "    def handling_outliers(self,datas,columns):\n",
    "        for i in columns:\n",
    "            q1=datas[i].quantile(0.25)\n",
    "            q3=datas[i].quantile(0.75)\n",
    "            IQR=q3-q1\n",
    "            upper_lim=q3+1.5*IQR\n",
    "            lower_lim=q1-1.5*IQR\n",
    "            datas[i]=datas[i].apply(lambda x:upper_lim if x>upper_lim else lower_lim if x<lower_lim else x)\n",
    "        \n",
    "        return datas\n",
    "\n",
    "    def feature_E(self):\n",
    "        try:\n",
    "            df=pd.read_csv(self.config.data_dir)\n",
    "            logger.info(f\"the columns are {df.columns} and shape is : {df.shape}\")\n",
    "\n",
    "            df.drop_duplicates(inplace=True)\n",
    "            logger.info(f\"duplicated data number: {df.duplicated().sum()}. Duplicates deletion completed\")\n",
    "\n",
    "            df['VehicleYear']=df['VehicleYear'].astype('str')\n",
    "            logger.info(f\"vehicleYear dtype changed to object\")\n",
    "            df['ClaimYear']=df['ClaimDate'].str.split('-').str[0]\n",
    "            logger.info(f\"feature ClaimYear extracted from ClaimDate\")\n",
    "            df['MaintenanceFrequency']=df['MaintenanceFrequency'].str.replace('hours','').astype('Int64')\n",
    "            logger.info(f\"in feature MaintenanceFrequency hours is removed and dtype changed to Int64\")\n",
    "            df['PreviousFailures']=df['PreviousFailures'].astype('Int64')\n",
    "            logger.info(f\"in feature PreviousFailures dtype changed to Int64\")\n",
    "\n",
    "            for col in df.columns:\n",
    "                self.Random_Sample_imputation(data=df, feature=col)\n",
    "            logger.info(f\"replacing null values with random imputation completed...\")\n",
    "\n",
    "            self.handling_outliers(datas=df,columns=['HoursOfOperation','SettlementAmount','MaintenanceFrequency','PreviousFailures'])\n",
    "            logger.info(f\"Outlier handling completed...\")\n",
    "\n",
    "            df['WarrantyStatus'].replace({'Out of Warranty':'out_of_warranty','In Warranty':'in_warranty'},inplace=True)\n",
    "            logger.info(f\"warranty status values renamed as {df['WarrantyStatus'].unique()}\")\n",
    "            \n",
    "            df=pd.get_dummies(df, columns=['VehicleModel','VehicleYear','PartName','SupplierName','EnvironmentCondition','OperationalIntensity','WarrantyStatus','ClaimYear'], drop_first=True)\n",
    "            df['pass_fail'].replace({'pass': 1, 'fail': 0}, inplace=True)    \n",
    "            logger.info(f\"implementation of pd.dummies completed... \")\n",
    "\n",
    "            df.drop(['ClaimDate'],axis=1,inplace=True)\n",
    "            logger.info(f\"features dropped are : ClaimDate \")\n",
    "\n",
    "            save_path = Path(\"artifacts/data_transformation/FE_data.csv\")\n",
    "            logger.info(f\"Saving file to {save_path}\")\n",
    "            df.to_csv(save_path, index=False)\n",
    "\n",
    "            logger.info(f\"Feature Engineering Completed and file saved Successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self,config: Data_Transformation_Config):\n",
    "        self.config=config\n",
    "\n",
    "    def train_test_split(self):\n",
    "        try:\n",
    "            \n",
    "            df=pd.read_csv(\"artifacts/data_transformation/FE_data.csv\")\n",
    "            logger.info(\"data imported succcessfully\")\n",
    "\n",
    "            train,test=train_test_split(df)\n",
    "            logger.info(\"train test split completed\")\n",
    "\n",
    "            train.to_csv(os.path.join(self.config.root_dir,\"train.csv\"),index=False)\n",
    "            test.to_csv(os.path.join(self.config.root_dir,\"test.csv\"),index=False)\n",
    "            logger.info(\"train and test file saved completed\")\n",
    "\n",
    "            logger.info(f\"train shape: {train.shape} and test shape : {test.shape} \")\n",
    "\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-29 16:50:12,345: INFO: common: yaml file {'artifacts_root': 'artifacts', 'data_ingestion': {'root_dir': 'artifacts/data_ingestion', 'source_URL': 'https://github.com/hrishikesh147/MLProject/raw/main/part_namesz.zip', 'local_data_file': 'artifacts/data_ingestion/zipped_data.zip', 'unzip_data': 'artifacts/data_ingestion/data'}, 'data_validation': {'root_dir': 'artifacts/data_validation', 'status_file': 'artifacts/data_validation/status.txt', 'unzip_data_dir': 'artifacts/data_ingestion/data/part_names.csv'}, 'data_transformation': {'root_dir': 'artifacts/data_transformation', 'data_transformation_input': 'artifacts/data_ingestion/data'}} loaded successfully]\n",
      "[2024-03-29 16:50:12,380: INFO: common: yaml file {'COLUMNS': {'VehicleModel': 'object', 'VehicleYear': 'int64', 'PartName': 'object', 'HoursOfOperation': 'float64', 'SupplierName': 'object', 'ClaimDate': 'object', 'SettlementAmount': 'float64', 'MaintenanceFrequency': 'object', 'EnvironmentCondition': 'object', 'OperationalIntensity': 'object', 'WarrantyStatus': 'object', 'PreviousFailures': 'int64', 'pass_fail': 'object'}, 'TARGET_COLUMN': {'name': 'pass_fail'}} loaded successfully]\n",
      "[2024-03-29 16:50:12,400: INFO: common: yaml file {'kay1': 'value1'} loaded successfully]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'artifacts_root'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     dt\u001b[38;5;241m.\u001b[39mtrain_test_split()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     conf\u001b[38;5;241m=\u001b[39m\u001b[43mConfigurationManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     con\u001b[38;5;241m=\u001b[39mconf\u001b[38;5;241m.\u001b[39mdata_transformed_configuration()  \n\u001b[0;32m      4\u001b[0m     fe\u001b[38;5;241m=\u001b[39mFeature_Engineering(con)\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36mConfigurationManager.__init__\u001b[1;34m(self, config_read, schema_read, params_read)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\u001b[38;5;241m=\u001b[39mread_yaml(schema_read),\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m=\u001b[39mread_yaml(params_read)\n\u001b[1;32m----> 9\u001b[0m create_directories([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43martifacts_root\u001b[49m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'artifacts_root'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conf=ConfigurationManager()\n",
    "    con=conf.data_transformed_configuration()  \n",
    "    fe=Feature_Engineering(con)\n",
    "    fe1=fe.feature_E()\n",
    "    dt=DataTransformation(con)\n",
    "    dt.train_test_split()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
